import { useRef, useState, useEffect } from "react";
import { useNavigate } from "react-router-dom";
import MainPresenter from "./MainPresenter";

const MainContainer = () => {
  const navigate = useNavigate();
  const [isRecording, setIsRecording] = useState(false);
  const [text, setText] = useState("");
  const [keywords, setKeywords] = useState([]);
  const [badwords, setBadwords] = useState([]);
  const [isAutoRecording, setIsAutoRecording] = useState(false);
  const [habitCandidates, setHabitCandidates] = useState([]);

  const mediaRecorder = useRef(null);
  const recordedChunksRef = useRef([]);
  const recordingTimeoutRef = useRef(null);
  const streamRef = useRef(null);
  const isAutoRecordingRef = useRef(false);

  const isSilent = async (blob) => {
    return new Promise((resolve) => {
      const audioContext = new AudioContext();
      const reader = new FileReader();
      reader.onloadend = async () => {
        try {
          await audioContext.resume();
          const buffer = await audioContext.decodeAudioData(reader.result);
          const input = buffer.getChannelData(0);
          const avgVolume = input.reduce((sum, val) => sum + Math.abs(val), 0) / input.length;
          console.log("ğŸ§ í‰ê·  ë³¼ë¥¨:", avgVolume);
          resolve(avgVolume < 0.006);
        } catch (error) {
          console.error("âŒ ì˜¤ë””ì˜¤ ë””ì½”ë”© ì‹¤íŒ¨:", error);
          resolve(false);
        }
      };
      reader.readAsArrayBuffer(blob);
    });
  };

  const analyzeInBackground = async (blob) => {
    const formData = new FormData();
    formData.append("file", blob, "audio.webm");
    try {
      const res = await fetch("http://localhost:8000/analyze-audio", {
        method: "POST",
        body: formData,
      });
      const data = await res.json();
      if (!res.ok) {
        console.error("ì„œë²„ ì˜¤ë¥˜:", data.error);
        return;
      }
      setText(data.transcribed_text || "");
      console.log("Test, --- ì¸ì‹ëœ í…ìŠ¤íŠ¸:", data.transcribed_text || "(ì—†ìŒ)");
      setKeywords(data.detected_keywords || []);
      setBadwords(data.detected_badwords || []);
      fetchHabitCandidates();
    } catch (err) {
      console.error("ë¶„ì„ ì‹¤íŒ¨:", err);
    }
  };

  const fetchHabitCandidates = async () => {
    try {
      const res = await fetch("http://localhost:8000/habit/candidates");
      const data = await res.json();
      setHabitCandidates(data.habit_candidates);
      console.log(data.habit_candidates)
    } catch (err) {
      console.error("ë§ë²„ë¦‡ í›„ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨:", err);
    }
  };

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;
      const mimeType = "audio/webm;codecs=opus";

      if (!MediaRecorder.isTypeSupported(mimeType)) {
        alert("WebM ì½”ë±ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤.");
        isAutoRecordingRef.current = false;
        setIsAutoRecording(false);
        return;
      }

      recordedChunksRef.current = [];
      mediaRecorder.current = new MediaRecorder(stream, { mimeType });

      mediaRecorder.current.ondataavailable = (e) => {
        if (e.data.size > 0) {
          recordedChunksRef.current.push(e.data);
        }
      };

      mediaRecorder.current.onstop = async () => {
        console.log("ë…¹ìŒ ì¢…ë£Œ")
        const blob = new Blob(recordedChunksRef.current, { type: mimeType });
        console.log("Blob ìƒì„± ì™„ë£Œ:", blob.size, "bytes");
        recordedChunksRef.current = [];
        if (blob.size >= 5000) {
          const silent = await isSilent(blob);
          console.log("ë¬´ìŒ ì²˜ë¦¬ ì—¬ë¶€: ", silent)
          if (!silent) await analyzeInBackground(blob);
        }
        if (isAutoRecordingRef.current) {
          console.log("---ìë™ ë…¹ìŒ ì¬ì‹œì‘---");
          if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop());
            streamRef.current = null;
          }
          setTimeout(() => {
            startRecording();
          }, 100);
        }
      };

      mediaRecorder.current.start();
      console.log("---ë…¹ìŒ ì‹œì‘ ---")
      setIsRecording(true);
      recordingTimeoutRef.current = setTimeout(() => {
        if (mediaRecorder.current?.state === "recording") {
          mediaRecorder.current.stop();
          setIsRecording(false);
        }
      }, 6000);
    } catch (err) {
      isAutoRecordingRef.current = false;
      setIsAutoRecording(false);
      alert("ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.");
    }
  };

  const stopRecording = () => {
    if (mediaRecorder.current?.state === "recording") {
      mediaRecorder.current.stop();
      setIsRecording(false);
    }
    if (recordingTimeoutRef.current) {
      clearTimeout(recordingTimeoutRef.current);
    }
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    isAutoRecordingRef.current = false;
    setIsAutoRecording(false);
  };

  const toggleAutoRecording = () => {
    if (isAutoRecording) {
      stopRecording();
    } else {
      isAutoRecordingRef.current = true;
      setIsAutoRecording(true);
      startRecording();
    }
  };

  useEffect(() => {
    fetchHabitCandidates();
    return () => {
      if (recordingTimeoutRef.current) clearTimeout(recordingTimeoutRef.current);
      if (mediaRecorder.current?.state === "recording") mediaRecorder.current.stop();
      if (streamRef.current) streamRef.current.getTracks().forEach(track => track.stop());
    };
  }, []);

  return (
    <MainPresenter
      isRecording={isRecording}
      startRecording={toggleAutoRecording}
      stopRecording={stopRecording}
      text={text}
      keywords={keywords}
      badwords={badwords}
      navigate={navigate}
      isAutoRecording={isAutoRecording}
      habitCandidates={habitCandidates}
    />
  );
};

export default MainContainer;
